
In order to verify our two hypothesis, two types of tests were performed:

\begin{itemize}
	\item {\bf Performance:}
		The implementations of the proposed heuristics were run against the whole
		benchamrk set seperately.
		Additionally, the original SP implementation\footnote{
			Original-SP denotes the up-to-date implementation of \cite{devriendt2012symmetry} with
			the optimizations enabled; in their paper, this version is denoted $SP^{opt}$.
		}was re-run, such that we could compare their
		results to ours.

	\item {\bf Symmetry activity test:}
		We chose several tests (the justification for this set can be found
		\ref{ssec:sym_act_test_cases}) and ran them against the original SP implementation and the
		implementation of the branching heuristic based on symmetry activity (with lookahead).
		We recorded the number of symmetries active during the search process.

\end{itemize}

The \emph{symmetry activity tests} allows us to verify that our heuristics actually succeed in
keeping more heuristics 'active' during searching.
The performance results can then be used to test the hypothesis
keeping more symmetries active during searching leads to a more effective search process,
because more variables can be propagated.

\subsection{Symmetry Activity Test Cases}
\label{ssec:sym_act_test_cases}
	The tests for the symmetry activity were chosen based on performance tests.
	We focused on three tests\todo{Be specific, what tests?}:
	\begin{enumerate}
		\item A test where the original-SP needed less decisions than our implementation
		\item A test where our implementation needed less decisions than original SP
		\item A test where both implementations performed similar
	\end{enumerate}

	We chose these tests because we hypothesised that the cases where out implementation was fast,
	were the cases where it had succeeded to keep a lot of symmetries active,
	whereas the cases where it failed to do so, would be slower due to the overhead of the lookahead.

\subsection{The Test Suite}
\label{ssec:benchmark}
	The benchmark set we ran our tests against is the same set as used by Devriendt~\cite{devriendt2012symmetry}.
	This benchmark was composed to measure the performance of their dynamic symmetry
	breaking algorithm and to verify that their optimization heuristics worked.
	Because we are benchmarking heuristics that were build on top of their implementation, their
	benchmark was an obvious choice.

	The benchmark contains both satisfiable and unsatisfiable instances of several SAT problems.
	\todo{more justification of benchmark?}
