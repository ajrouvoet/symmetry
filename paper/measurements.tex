
In order to verify our two hypothesis, two types of tests were performed:

\begin{itemize}
	\item {\bf Performance:}
		The implementations of the proposed heuristics were run against the whole
		benchmark set separately.
		Additionally, the original SP implementation\footnote{
			Original-SP denotes the up-to-date implementation of \cite{devriendt2012symmetry}
			without the inverting symmetry optimization enabled; in their paper, this version is denoted $SP^{reg}$.
		}was re-run, such that we could compare their results to ours.

	\item {\bf Symmetry activity test:}
		We chose several tests (the justification for this set can be found
		\ref{ssec:sym_act_test_cases}) and ran them against the original SP implementation and the
		implementation of the branching heuristic based on symmetry activity (with lookahead).
		We recorded the number of symmetries active during the search process.

\end{itemize}

The \emph{symmetry activity tests} allows us to verify that our heuristics actually succeed in
keeping more heuristics 'active' during searching.
The performance results can then be used to test the hypothesis
keeping more symmetries active during searching leads to a more effective search process,
because more variables can be propagated.

\subsection{Symmetry Activity Test Cases}
\label{ssec:sym_act_test_cases}
	In order to verify that the heuristics increased the number of active symmetries during search,
	we ran the benchmark, and logged the amount of active symmetries in every iteration of the
	search algorithm.
	We then calculated the average number of active symmetries for each test case for the different
	heuristics and the original SP implementations (regular and optimized).

\subsection{Benchmark}
\label{ssec:benchmark}
	The benchmark set we ran our tests against is the same set as used by Devriendt~\cite{devriendt2012symmetry}.
	This benchmark was composed to measure the performance of their dynamic symmetry
	breaking algorithm and to verify that their optimization heuristics worked.
	Because we are benchmarking heuristics that were build on top of their implementation, their
	benchmark was an obvious choice.

	The benchmark contains both satisfiable and unsatisfiable instances of several SAT problems.
	This set of problems consists of field-programmable integrated circuit wire routing (\emph{fpga} and
	\emph{chnl})~\cite{nam2004comparative}, Urquhart's problems (\emph{Urq})~\cite{urquhart1987hard},
	pigeonhole problems (\emph{hole}), encryption problems (\emph{xorchain}) and the \emph{battleships}
	puzzle problems~\cite{sevenster2004battleships}.
	Problem sets \emph{Urq} and \emph{x} contain only problems with inverting symmetries, while the
	others like \emph{chnl}, \emph{fpga} and \emph{battleship} do not contain this type of symmetry
	at all\cite{devriendt2012symmetry}.

	All tests were run with a 15 minute timeout.
	If all implementations timed out, the results are omitted from the table.

	In our results we are focusing on the total amount of decisions taken.
	This is a good measure of the effectiveness of the search.
	Another option would be CPU time, but apart from the effectiveness of the search, this measure
	also includes the quality of the implementation and the specifics of the machine on which it was
	run.
	Due to the large number of tests, it was not possible to run all tests on one machine, which
	would make CPU time not comparable over different tests.
	Furthermore, we did not have the time to optimize the implementations of the heuristics for
	speed, such that the implementation quality would hide the actual search performance.
